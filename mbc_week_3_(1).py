# -*- coding: utf-8 -*-
"""MBC Week 3 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O75Ggq0IQgfI-KhM5amVO50ofFzi1C_G

# TUGAS MBC WEEK 3

- NAMA : BENEDICT BRIAN JOEL PURBA
- NIM : 103052300066
- NO : 2517

## DATASET
"""

import kagglehub
shaunthesheep_microsoft_catsvsdogs_dataset_path = kagglehub.dataset_download('shaunthesheep/microsoft-catsvsdogs-dataset')

print('Data source import complete.')

import zipfile
import os

with zipfile.ZipFile("archive (16).zip", 'r') as zip_ref:
    zip_ref.extractall("cats_vs_dogs")


if os.path.exists("cats_vs_dogs"):
    print("Data ready!")

for root, dirs, files in os.walk("cats_vs_dogs"):
    print(root, len(files), "files")

"""## VISUALISASI"""

import matplotlib.pyplot as plt
from PIL import Image
import os

cats_folder_eda = "cats_vs_dogs/PetImages/Cat"
dogs_folder_eda = "cats_vs_dogs/PetImages/Dog"

# untuk samplingnya set jadi 5
cat_files_eda = [os.path.join(cats_folder_eda, f) for f in os.listdir(cats_folder_eda) if f.endswith('.jpg') or f.endswith('.png')][:5] # Take 5 samples
dog_files_eda = [os.path.join(dogs_folder_eda, f) for f in os.listdir(dogs_folder_eda) if f.endswith('.jpg') or f.endswith('.png')][:5] # Take 5 samples

print("Sample Cat Images:")
plt.figure(figsize=(15, 5))
for i, cat_file in enumerate(cat_files_eda):
    try:
        img = Image.open(cat_file)
        plt.subplot(1, 5, i + 1)
        plt.imshow(img)
        plt.title(f"Cat {i+1}")
        plt.axis('off')
    except Exception as e:
        print(f"Could not open image {cat_file}: {e}")
plt.show()

print("\nSample Dog Images:")
plt.figure(figsize=(15, 5))
for i, dog_file in enumerate(dog_files_eda):
    try:
        img = Image.open(dog_file)
        plt.subplot(1, 5, i + 1)
        plt.imshow(img)
        plt.title(f"Dog {i+1}")
        plt.axis('off')
    except Exception as e:
        print(f"Could not open image {dog_file}: {e}")
plt.show()

print("\nEDA complete: Displayed sample images from Cats and Dogs folders.")

from PIL import Image, UnidentifiedImageError

def remove_corrupted(folder):
    removed = 0
    for fname in os.listdir(folder):
        fpath = os.path.join(folder, fname)
        try:
            img = Image.open(fpath)
            img.verify()
        except (IOError, UnidentifiedImageError, Exception):
            os.remove(fpath)
            removed += 1
    return removed

cats_folder = "cats_vs_dogs/PetImages/Cat"
dogs_folder = "cats_vs_dogs/PetImages/Dog"

print("Removed cats:", remove_corrupted(cats_folder))
print("Removed dogs:", remove_corrupted(dogs_folder))

from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os

base_dir = "cats_vs_dogs/PetImages"
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')

# Create train and validation directories if they don't exist
os.makedirs(train_dir, exist_ok=True)
os.makedirs(validation_dir, exist_ok=True)
os.makedirs(os.path.join(train_dir, 'Cat'), exist_ok=True)
os.makedirs(os.path.join(train_dir, 'Dog'), exist_ok=True)
os.makedirs(os.path.join(validation_dir, 'Cat'), exist_ok=True)
os.makedirs(os.path.join(validation_dir, 'Dog'), exist_ok=True)

# Dummy data splitting (replace with actual splitting logic if needed)
# This is a simplified example and might not represent a good split ratio
import shutil

cat_files = os.listdir(os.path.join(base_dir, 'Cat'))
dog_files = os.listdir(os.path.join(base_dir, 'Dog'))

for fname in cat_files[:1000]:
    src = os.path.join(base_dir, 'Cat', fname)
    dst = os.path.join(train_dir, 'Cat', fname)
    if os.path.exists(src):
        shutil.copyfile(src, dst)

for fname in cat_files[1000:1200]:
    src = os.path.join(base_dir, 'Cat', fname)
    dst = os.path.join(validation_dir, 'Cat', fname)
    if os.path.exists(src):
        shutil.copyfile(src, dst)

for fname in dog_files[:1000]:
    src = os.path.join(base_dir, 'Dog', fname)
    dst = os.path.join(train_dir, 'Dog', fname)
    if os.path.exists(src):
        shutil.copyfile(src, dst)

for fname in dog_files[1000:1200]:
    src = os.path.join(base_dir, 'Dog', fname)
    dst = os.path.join(validation_dir, 'Dog', fname)
    if os.path.exists(src):
        shutil.copyfile(src, dst)


# Instantiate ImageDataGenerator for training data with augmentation and rescaling
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Instantiate ImageDataGenerator for validation data with only rescaling
validation_datagen = ImageDataGenerator(rescale=1./255)

# Create training data generator
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150), # Target size for the images
    batch_size=32,
    class_mode='binary', # For binary classification
    shuffle=True
)

# Create validation data generator
validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary',
    shuffle=False # No need to shuffle validation data
)

print("Data generators set up successfully.")

"""## cnn model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# 2. Initialize a Sequential model.
custom_cnn_model = Sequential()

# 3. Add the first convolutional layer
custom_cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
# 4. Add a MaxPooling2D layer
custom_cnn_model.add(MaxPooling2D((2, 2)))

# 5. Add more pairs of convolutional and max pooling layers
custom_cnn_model.add(Conv2D(64, (3, 3), activation='relu'))
custom_cnn_model.add(MaxPooling2D((2, 2)))

custom_cnn_model.add(Conv2D(128, (3, 3), activation='relu'))
custom_cnn_model.add(MaxPooling2D((2, 2)))

custom_cnn_model.add(Conv2D(128, (3, 3), activation='relu'))
custom_cnn_model.add(MaxPooling2D((2, 2)))

# 6. Add a Flatten layer
custom_cnn_model.add(Flatten())

# 7. Add one or more Dense (fully connected) layers
custom_cnn_model.add(Dense(512, activation='relu'))

# 8. Add the final output Dense layer
custom_cnn_model.add(Dense(1, activation='sigmoid'))

# 9. Print a summary of the model
custom_cnn_model.summary()

from sklearn.metrics import f1_score
import numpy as np

# 1. Compile the custom_cnn_model
custom_cnn_model.compile(optimizer='adam',
                         loss='binary_crossentropy',
                         metrics=['accuracy'])

# 2. Train the compiled model
epochs = 15
steps_per_epoch = train_generator.samples // train_generator.batch_size
validation_steps = validation_generator.samples // validation_generator.batch_size

history = custom_cnn_model.fit(
    train_generator,
    steps_per_epoch=steps_per_epoch,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=validation_steps
)

# 3. Evaluate the trained model's performance
evaluation_results = custom_cnn_model.evaluate(validation_generator, steps=validation_steps)
print(f"Validation Loss: {evaluation_results[0]:.4f}")
print(f"Validation Accuracy: {evaluation_results[1]:.4f}")

# 4. Calculate the F1 score
# Get predictions from the validation generator
validation_generator.reset() # Reset the generator to ensure order
y_pred_probs = custom_cnn_model.predict(validation_generator, steps=validation_steps)
y_pred = (y_pred_probs > 0.5).astype(int) # Convert probabilities to binary class labels

# Get the true labels from the validation generator
y_true = validation_generator.classes[:validation_steps * validation_generator.batch_size]

# Calculate F1 score
f1 = f1_score(y_true, y_pred)

# 5. Print the evaluation metrics
print(f"Validation F1 Score: {f1:.4f}")

"""## Choose and prepare a pre-trained model

### Subtask:
Select a pre-trained model (e.g., VGG16, ResNet50) and load it without the top classification layer.

"""

from tensorflow.keras.applications import VGG16

# Load the VGG16 model with pre-trained ImageNet weights
# Exclude the top classification layer (include_top=False)
# Specify the input shape to match the image data generators (150x150 with 3 color channels)
pretrained_model = VGG16(weights='imagenet',
                         include_top=False,
                         input_shape=(150, 150, 3))

# Print a summary of the loaded pre-trained model
pretrained_model.summary()

"""## Fine-tune the pre-trained model

### Subtask:
Add new classification layers to the pre-trained model and fine-tune it on the "Cats vs. Dogs" dataset using transfer learning.

"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.optimizers import Adam

# 2. Set the trainable attribute of the layers in the pretrained_model to False
for layer in pretrained_model.layers:
    layer.trainable = False

# 3. Create a new Sequential model
transfer_model = Sequential()

# 4. Add the pretrained_model as the first layer to the new Sequential model
transfer_model.add(pretrained_model)

# 5. Add a Flatten layer
transfer_model.add(Flatten())

# 6. Add one or more Dense layers with a 'relu' activation function and a Dropout layer
transfer_model.add(Dense(256, activation='relu'))
transfer_model.add(Dropout(0.5)) # Adding a dropout layer for regularization

# 7. Add the final output Dense layer
transfer_model.add(Dense(1, activation='sigmoid'))

# 8. Compile the new model
transfer_model.compile(optimizer=Adam(learning_rate=0.0001), # Using Adam optimizer with a lower learning rate
                       loss='binary_crossentropy',
                       metrics=['accuracy'])

# Print a summary of the new model
transfer_model.summary()

# 9. Train the compiled model
history_pretrained = transfer_model.fit(
    train_generator,
    steps_per_epoch=steps_per_epoch,
    epochs=epochs, # Using the previously defined epochs
    validation_data=validation_generator,
    validation_steps=validation_steps
)

from sklearn.metrics import f1_score
import numpy as np

# 1. Evaluate the transfer_model on the validation_generator
evaluation_results_transfer = transfer_model.evaluate(validation_generator, steps=validation_steps)

# 2. Print the validation loss and validation accuracy
print(f"Transfer Model Validation Loss: {evaluation_results_transfer[0]:.4f}")
print(f"Transfer Model Validation Accuracy: {evaluation_results_transfer[1]:.4f}")

# 3. Reset the validation_generator to ensure the order of samples is consistent for predictions
validation_generator.reset()

# 4. Get the predicted probabilities from the transfer_model
y_pred_probs_transfer = transfer_model.predict(validation_generator, steps=validation_steps)

# 5. Convert the predicted probabilities to binary class labels
y_pred_transfer = (y_pred_probs_transfer > 0.5).astype(int)

# 6. Get the true labels from the validation_generator
y_true_transfer = validation_generator.classes[:validation_steps * validation_generator.batch_size]

# 7. Calculate the F1 score
f1_transfer = f1_score(y_true_transfer, y_pred_transfer)

# 8. Print the calculated validation F1 score
print(f"Transfer Model Validation F1 Score: {f1_transfer:.4f}")

custom_cnn_accuracy = history.history['val_accuracy'][-1]
custom_cnn_f1 = f1

transfer_model_accuracy = history_pretrained.history['val_accuracy'][-1]
transfer_model_f1 = f1_transfer

print("Performance Comparison:")
print("-" * 30)
print(f"Custom CNN Model:")
print(f"  Validation Accuracy: {custom_cnn_accuracy:.4f}")
print(f"  Validation F1 Score: {custom_cnn_f1:.4f}")
print("-" * 30)
print(f"Fine-tuned Pre-trained Model (VGG16):")
print(f"  Validation Accuracy: {transfer_model_accuracy:.4f}")
print(f"  Validation F1 Score: {transfer_model_f1:.4f}")
print("-" * 30)

if transfer_model_accuracy > custom_cnn_accuracy:
    best_model = transfer_model
    model_name = "Fine-tuned Pre-trained Model"
    filename = 'best_transfer_model.h5'
elif custom_cnn_accuracy > transfer_model_accuracy:
    best_model = custom_cnn_model
    model_name = "Custom CNN Model"
    filename = 'best_custom_cnn_model.h5'
else:
    # If accuracies are equal, compare F1 scores
    if transfer_model_f1 > custom_cnn_f1:
        best_model = transfer_model
        model_name = "Fine-tuned Pre-trained Model"
        filename = 'best_transfer_model.h5'
    elif custom_cnn_f1 > transfer_model_f1:
        best_model = custom_cnn_model
        model_name = "Custom CNN Model"
        filename = 'best_custom_cnn_model.h5'
    else:
        # If both metrics are equal, default to the transfer model
        best_model = transfer_model
        model_name = "Fine-tuned Pre-trained Model (Default)"
        filename = 'best_transfer_model.h5'

# Save the best performing model to a file
best_model.save(filename)

# Print a confirmation message
print(f"The best performing model ({model_name}) has been saved to {filename}")

# 1. Summarize the architecture of the custom CNN model
print("Custom CNN Model Architecture Summary:")
print("- Input Layer: (150, 150, 3)")
print("- Convolutional Layers: Four pairs of Conv2D and MaxPooling2D layers with filter sizes 32, 64, 128, and 128. ReLU activation used.")
print("- Flatten Layer: Connects convolutional base to dense layers.")
print("- Dense Layers: One hidden layer with 512 units and ReLU activation, followed by an output layer with 1 unit and Sigmoid activation for binary classification.")
print(f"- Total Parameters: {custom_cnn_model.count_params():,}")
print("-" * 30)

# 2. Summarize the architecture of the fine-tuned pre-trained model
print("Fine-tuned Pre-trained Model Architecture Summary:")
print("- Base Model: VGG16, pre-trained on ImageNet, excluding the top classification layer.")
print("- Frozen Layers: All layers in the VGG16 base model were set to non-trainable.")
print("- Added Classification Layers:")
print("  - Flatten Layer.")
print("  - Dense Layer: One hidden layer with 256 units and ReLU activation.")
print("  - Dropout Layer: For regularization (Dropout rate 0.5).")
print("  - Output Layer: One unit with Sigmoid activation for binary classification.")
print(f"- Total Trainable Parameters: {transfer_model.count_params() - sum(layer.count_params() for layer in pretrained_model.layers if not layer.trainable):,}")
print("-" * 30)

# 3. Describe the training process for both models
print("Training Process Summary:")
print("- Optimizer: Adam was used for both models.")
print("- Loss Function: Binary Crossentropy was used for both binary classification tasks.")
print(f"- Epochs: Both models were trained for {epochs} epochs.")
print("- Data Generators: ImageDataGenerators were used to load and preprocess images in batches.")
print("- Data Augmentation: Applied to the training data (rotation, shifting, shearing, zooming, horizontal flipping) for the custom CNN and the pre-trained model.")
print("- Validation Data: Used a separate ImageDataGenerator with only rescaling.")
print(f"- Steps per epoch: {steps_per_epoch}")
print(f"- Validation steps: {validation_steps}")
print("-" * 30)

# 4. Present the evaluation metrics
print("Evaluation Metrics:")
print("-" * 30)
print("Model             | Validation Accuracy | Validation F1 Score")
print("------------------|---------------------|--------------------")
print(f"Custom CNN        | {custom_cnn_accuracy:.4f}             | {custom_cnn_f1:.4f}")
print(f"Pre-trained (VGG16)| {transfer_model_accuracy:.4f}             | {transfer_model_f1:.4f}")
print("-" * 30)

# 5. Provide a comparative analysis
print("\nComparative Analysis:")
print("The fine-tuned pre-trained VGG16 model significantly outperformed the custom CNN model on the validation set.")
print(f"The pre-trained model achieved a validation accuracy of {transfer_model_accuracy:.4f} and an F1 score of {transfer_model_f1:.4f}, which are substantially higher than the custom CNN's accuracy of {custom_cnn_accuracy:.4f} and F1 score of {custom_cnn_f1:.4f}.")
print("This demonstrates the effectiveness of transfer learning. The VGG16 model, having been pre-trained on the massive ImageNet dataset, learned robust feature extractors that are highly relevant for image classification tasks, including distinguishing between cats and dogs.")
print("By leveraging these pre-learned features and only training the relatively small number of new classification layers, the transfer model could achieve high performance even with a relatively limited amount of training data from the 'Cats vs. Dogs' dataset.")
print("In contrast, the custom CNN model was trained from scratch. Without the benefit of prior learning on a large dataset, it required more data and potentially a more complex architecture or extensive hyperparameter tuning to develop effective features for this task. Its lower performance indicates it struggled more to learn these features from the smaller dataset.")
print("The results highlight the significant advantage of using transfer learning for image classification problems, especially when the available dataset is not extremely large.")

"""## Summary:

### Data Analysis Key Findings

*   The custom CNN model achieved a validation accuracy of 0.5755 and a validation F1 score of 0.6823 after training for 15 epochs.
*   The fine-tuned pre-trained VGG16 model achieved a significantly higher validation accuracy of 0.8542 and a validation F1 score of 0.8579 after training for 15 epochs with its base layers frozen.
*   The pre-trained VGG16 model, even with its base layers frozen and only training the new classification layers, demonstrated substantially better performance than the custom CNN trained from scratch on the same dataset.
*   The fine-tuned pre-trained model was determined to be the best performing model and was saved to `best_transfer_model.h5`.

### Insights or Next Steps

*   Transfer learning using a pre-trained model like VGG16 is a highly effective approach for image classification tasks on relatively small datasets, significantly outperforming custom models trained from scratch.
*   Further improvements to the custom CNN could potentially be achieved with more extensive hyperparameter tuning, a deeper architecture, or training for more epochs, but it would likely require significantly more computational resources and data compared to the transfer learning approach to reach comparable performance.

"""

import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import numpy as np
import os

# Load the best performing model (assuming it was saved as 'best_transfer_model.h5')
try:
    best_model = load_model('best_transfer_model.h5')
    print("Best model loaded successfully.")
except Exception as e:
    print(f"Error loading model: {e}")
    print("Please ensure 'best_transfer_model.h5' exists and is the correct model file.")

# Define paths to the Cat and Dog folders for sampling
cats_folder_sample = "cats_vs_dogs/PetImages/Cat"
dogs_folder_sample = "cats_vs_dogs/PetImages/Dog"

# Get a list of image files (taking a few for sampling)
# We'll take a few more samples to have a better visualization
num_samples = 10
cat_files_sample = [os.path.join(cats_folder_sample, f) for f in os.listdir(cats_folder_sample) if f.endswith('.jpg') or f.endswith('.png')][:num_samples]
dog_files_sample = [os.path.join(dogs_folder_sample, f) for f in os.listdir(dogs_folder_sample) if f.endswith('.jpg') or f.endswith('.png')][:num_samples]

# Combine the sample image paths
all_sample_files = cat_files_sample + dog_files_sample

# Shuffle the sample files for a mixed display
import random
random.shuffle(all_sample_files)

# Define the target size that the model expects (from data generators setup)
target_size_model = (150, 150)

# Visualize the predictions
plt.figure(figsize=(20, 10))
for i, img_path in enumerate(all_sample_files):
    try:
        # Load and preprocess the image
        img = image.load_img(img_path, target_size=target_size_model)
        img_array = image.img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0) # Add batch dimension
        img_array /= 255.0 # Rescale the image like in the data generators

        # Make a prediction
        prediction = best_model.predict(img_array)

        # Get the predicted class label
        # Assuming binary classification where 0 is Cat and 1 is Dog
        if prediction[0][0] > 0.5:
            predicted_class = "Dog"
            confidence = prediction[0][0]
        else:
            predicted_class = "Cat"
            confidence = 1 - prediction[0][0] # Confidence for the predicted class

        # Display the image and prediction
        plt.subplot(2, num_samples // 2, i + 1)
        plt.imshow(img)
        plt.title(f"Predicted: {predicted_class}\nConfidence: {confidence:.2f}")
        plt.axis('off')

    except Exception as e:
        print(f"Could not process image {img_path}: {e}")

plt.tight_layout()
plt.show()

print("\nVisualization of image identification results complete.")