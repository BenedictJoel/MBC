# -*- coding: utf-8 -*-
"""MBC WEEK 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ac50QFENoi0qEvPttw_TRwCULSLKfeX-

# **TUGAS MBC WEEK 4**

- NAMA : BENEDICT BRIAN JOEL PURBA
- NIM : 103052300066
- KODE : 2517

## EKSPERIMEN 2
"""

import os
import argparse
import random
import numpy as np
import pandas as pd
from dataclasses import dataclass
from typing import Dict, List


from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import f1_score, classification_report


import torch
import torch.nn as nn
from torch.utils.data import Dataset

from transformers import (
AutoTokenizer,
AutoModelForSequenceClassification,
DataCollatorWithPadding,
Trainer,
TrainingArguments,
EarlyStoppingCallback,
set_seed,
)

LABELS = ["SADNESS", "ANGER", "HOPE", "DISAPPOINTMENT", "SUPPORT"]
LABEL2ID: Dict[str, int] = {l: i for i, l in enumerate(LABELS)}
ID2LABEL: Dict[int, str] = {i: l for l, i in LABEL2ID.items()}

class TextClsDataset(Dataset):
  def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int):
    self.texts = texts
    self.labels = labels
    self.tokenizer = tokenizer
    self.max_length = max_length


    def __len__(self):
     return len(self.texts)


    def __getitem__(self, idx):
      text = str(self.texts[idx])
    enc = self.tokenizer(
    text,
    truncation=True,
    max_length=self.max_length,
    padding=False,
    )
    if self.labels is None:
      return {**enc}
    return {**enc, "labels": int(self.labels[idx])}

class WeightedCEWrapper:
  model: AutoModelForSequenceClassification
  class_weights: torch.Tensor


  def __call__(self, outputs, labels):
    # outputs.logits: (B, num_labels)
    loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(outputs.logits.device))
    loss = loss_fct(outputs.logits, labels)
    return loss

def compute_metrics(eval_pred):
  logits, labels = eval_pred
  preds = np.argmax(logits, axis=1)
  macro_f1 = f1_score(labels, preds, average='macro')
  return {"macro_f1": macro_f1}

def parse_args():
  parser = argparse.ArgumentParser()
  parser.add_argument("--data_dir", type=str, required=True,
  help="Directory containing train.csv, test.csv, sample_submission.csv")
  parser.add_argument("--model_name", type=str, default="indobenchmark/indobert-base-p2",
  help="HF model checkpoint or local path")
  parser.add_argument("--epochs", type=int, default=3)
  parser.add_argument("--batch_size", type=int, default=16)
  parser.add_argument("--lr", type=float, default=2e-5)
  parser.add_argument("--weight_decay", type=float, default=0.01)
  parser.add_argument("--seed", type=int, default=42)
  parser.add_argument("--val_size", type=float, default=0.1, help="Validation size for holdout split")
  parser.add_argument("--max_length", type=int, default=160)
  parser.add_argument("--gradient_accumulation", type=int, default=1)
  parser.add_argument("--warmup_ratio", type=float, default=0.06)
  parser.add_argument("--early_stopping_patience", type=int, default=2)
  parser.add_argument("--out_csv", type=str, default="submission.csv")
  return parser.parse_args()

"""
Transformer-from-scratch (PyTorch) â€” Step-by-step components for emotion classification
This script implements (and documents) the following components:
 1) Tokenization & Vector Embedding
 2) Positional Encoding
 3) Scaled Dot-Product Attention
 4) Multi-Head Attention
 5) Transformer Encoder Layer (self-attention + feed-forward)
 6) Small classification head and training utilities (example, not heavy training)

Usage:
  - Inspect the classes/functions and import them into your training script, OR
  - Run as a script to start a tiny demo / or to train (see arguments).
Dependencies:
  - torch, numpy, pandas, sklearn
  pip install torch numpy pandas scikit-learn
"""

import re
import math
import os
from collections import Counter
from typing import List, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import f1_score
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# -----------------------------
# 1) Simple Tokenizer & Vocab
# -----------------------------
def simple_tokenize(text: str) -> List[str]:
    # Lowercase and split on whitespace and punctuation (very simple)
    text = text.lower().strip()
    # keep emoticons / emojis as separate tokens is complex; this simple tokenizer keeps words and punctuation
    tokens = re.findall(r"\w+|[^\s\w]", text, flags=re.UNICODE)
    return tokens

def build_vocab(texts: List[str], max_vocab: int = 20000, min_freq: int = 1):
    counter = Counter()
    for t in texts:
        counter.update(simple_tokenize(str(t)))
    # keep tokens with min_freq and top-k
    tokens = [tok for tok, cnt in counter.most_common(max_vocab) if cnt >= min_freq]
    # special tokens -> PAD=0, UNK=1
    itos = ['<PAD>', '<UNK>'] + tokens
    stoi = {tok: i for i, tok in enumerate(itos)}
    return stoi, itos

def encode_text(text: str, stoi: dict, max_len: int = 64):
    toks = simple_tokenize(text)
    ids = [stoi.get(t, 1) for t in toks][:max_len]  # 1 = UNK
    attn = [1] * len(ids)
    # pad
    pad_len = max_len - len(ids)
    if pad_len > 0:
        ids = ids + [0] * pad_len
        attn = attn + [0] * pad_len
    return ids, attn

# -----------------------------
# 2) Positional Encoding (sinusoidal)
# -----------------------------
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 512):
        super().__init__()
        # create positional encoding matrix (1, max_len, d_model)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        if d_model % 2 == 1:
            # if odd, last column remains zero for cosine part
            pe[:, 1::2] = torch.cos(position * div_term[:-1])
        else:
            pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer("pe", pe)  # buffer, not a parameter

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len, :].to(x.device)

# -----------------------------
# 3) Scaled Dot-Product Attention
# -----------------------------
def scaled_dot_product_attention(q, k, v, mask=None):
    """
    q,k,v: (batch, heads, seq_len, head_dim)
    mask: (batch, 1, 1, seq_len) or (batch, 1, seq_len)
    returns: context (batch, heads, seq_len, head_dim), attn_weights (batch, heads, seq_len, seq_len)
    """
    d_k = q.size(-1)
    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (batch, heads, seq_q, seq_k)
    if mask is not None:
        # mask: 1 where valid, 0 where padding -> we want to set padding positions to large negative
        scores = scores.masked_fill(mask == 0, -1e9)
    attn = torch.softmax(scores, dim=-1)
    context = torch.matmul(attn, v)
    return context, attn

# -----------------------------
# 4) Multi-Head Attention
# -----------------------------
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads

        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.out = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)

    def split_heads(self, x):
        # x: (batch, seq_len, d_model) -> (batch, heads, seq_len, head_dim)
        batch_size, seq_len, _ = x.size()
        x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)
        return x.transpose(1, 2)

    def combine_heads(self, x):
        # x: (batch, heads, seq_len, head_dim) -> (batch, seq_len, d_model)
        x = x.transpose(1, 2).contiguous()
        batch_size, seq_len, _, _ = x.size()
        return x.view(batch_size, seq_len, self.d_model)

    def forward(self, x, mask=None):
        # x: (batch, seq_len, d_model)
        q = self.w_q(x)
        k = self.w_k(x)
        v = self.w_v(x)

        q = self.split_heads(q)
        k = self.split_heads(k)
        v = self.split_heads(v)
        # mask shape expected: (batch, 1, 1, seq_len) or broadcastable
        if mask is not None:
            mask = mask.unsqueeze(1).unsqueeze(2)  # (batch,1,1,seq_len)
        context, attn = scaled_dot_product_attention(q, k, v, mask=mask)
        context = self.combine_heads(context)
        out = self.out(context)
        out = self.dropout(out)
        return out, attn  # out: (batch, seq_len, d_model)

# -----------------------------
# 5) Feed Forward Network (Positionwise)
# -----------------------------
class FeedForward(nn.Module):
    def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.dropout(self.linear2(self.relu(self.linear1(x))))

# -----------------------------
# 6) Transformer Encoder Layer (Self-Attention + Add&Norm + FeedForward)
# -----------------------------
class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model: int, num_heads: int, d_ff: int = 2048, dropout: float = 0.1):
        super().__init__()
        self.mha = MultiHeadAttention(d_model, num_heads, dropout=dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.ff = FeedForward(d_model, d_ff, dropout=dropout)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # Self-attention block with residual and layer norm
        attn_out, attn_weights = self.mha(x, mask=mask)
        x = self.norm1(x + attn_out)
        ff_out = self.ff(x)
        x = self.norm2(x + ff_out)
        return x, attn_weights

# -----------------------------
# 7) Full Transformer Encoder (stack of layers)
# -----------------------------
class TransformerEncoder(nn.Module):
    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])

    def forward(self, x, mask=None):
        attn_list = []
        for layer in self.layers:
            x, attn = layer(x, mask=mask)
            attn_list.append(attn)
        return x, attn_list

# -----------------------------
# 8) Complete Model for Classification
# -----------------------------
class SimpleTransformerClassifier(nn.Module):
    def __init__(self, vocab_size: int, d_model: int = 128, max_len: int = 128, num_layers: int = 2, num_heads: int = 4, d_ff: int = 256, num_classes: int = 5, dropout: float = 0.1):
        super().__init__()
        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=0)
        self.pos_enc = PositionalEncoding(d_model, max_len=max_len)
        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff, dropout)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(d_model, num_classes)
        self.d_model = d_model

    def forward(self, input_ids, attention_mask):
        # input_ids: (batch, seq_len)
        x = self.token_emb(input_ids) * math.sqrt(self.d_model)  # scale embeddings
        x = self.pos_enc(x)
        x, attn = self.encoder(x, mask=attention_mask)
        # pool: mean over valid tokens (attention_mask)
        mask = attention_mask.unsqueeze(-1)  # (batch, seq_len, 1)
        x_masked = x * mask
        summed = x_masked.sum(1)
        counts = mask.sum(1).clamp(min=1e-9)
        pooled = summed / counts  # (batch, d_model)
        pooled = self.dropout(pooled)
        logits = self.classifier(pooled)
        return logits, attn

# -----------------------------
# 9) Small Dataset wrapper
# -----------------------------
class CommentDataset(Dataset):
    def __init__(self, texts: List[str], labels: List[int], stoi: dict, max_len: int = 128):
        self.texts = texts
        self.labels = labels
        self.stoi = stoi
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        ids, attn = encode_text(self.texts[idx], self.stoi, self.max_len)
        item = {
            "input_ids": torch.tensor(ids, dtype=torch.long),
            "attention_mask": torch.tensor(attn, dtype=torch.float),
        }
        if self.labels is not None:
            item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

# -----------------------------
# 10) Utility: collate_fn for DataLoader
# -----------------------------
def collate_fn(batch):
    input_ids = torch.stack([b["input_ids"] for b in batch], dim=0)
    attention_mask = torch.stack([b["attention_mask"] for b in batch], dim=0)
    labels = torch.stack([b["labels"] for b in batch], dim=0) if "labels" in batch[0] else None
    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}

# -----------------------------
# 11) Example training step (single epoch, small) - included as a reference
# -----------------------------
def train_epoch(model, dataloader, optimizer, device, loss_fn):
    model.train()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    for batch in dataloader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)
        optimizer.zero_grad()
        logits, _ = model(input_ids, attention_mask)
        loss = loss_fn(logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * input_ids.size(0)
        preds = logits.argmax(dim=1).detach().cpu().numpy().tolist()
        all_preds.extend(preds)
        all_labels.extend(labels.detach().cpu().numpy().tolist())
    avg_loss = total_loss / len(dataloader.dataset)
    f1 = f1_score(all_labels, all_preds, average="macro")
    return avg_loss, f1

# -----------------------------
# 12) Main demo when running this file directly
# -----------------------------
def demo_small_forward_pass(train_csv_path=None):
    # If a train csv path is given and exists, sample a few rows to build vocab and demo forward
    if train_csv_path and os.path.exists(train_csv_path):
        df = pd.read_csv(train_csv_path)
        sample_texts = df["text"].astype(str).tolist()[:200]  # take first 200 to build vocab
    else:
        sample_texts = [
            "makin dewasa makin sadar bahwa taktik pemerintahan itu..",
            "Membuktikan hal yg benar di pengadilan itu memang sulit",
            "RIP keadilan di Indonesia ðŸ¥€ðŸ¥€ðŸ¥€",
            "semoga ada keadilan dan harapan baru untuk kita semua"
        ]

    stoi, itos = build_vocab(sample_texts, max_vocab=5000)
    vocab_size = len(itos)
    print(f"Demo: built vocab size={vocab_size}")
    max_len = 32
    # Prepare a tiny batch
    inputs = [encode_text(t, stoi, max_len)[0] for t in sample_texts[:4]]
    masks = [encode_text(t, stoi, max_len)[1] for t in sample_texts[:4]]
    input_ids = torch.tensor(inputs, dtype=torch.long)
    attention_mask = torch.tensor(masks, dtype=torch.float)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = SimpleTransformerClassifier(vocab_size=vocab_size, d_model=64, max_len=max_len, num_layers=2, num_heads=4, d_ff=128, num_classes=5).to(device)
    model.eval()
    with torch.no_grad():
        logits, attn = model(input_ids.to(device), attention_mask.to(device))
    print("Logits shape:", logits.shape)
    if attn and len(attn) > 0:
        print("Attention from last layer shape:", attn[-1].shape)  # (batch, heads, seq_len, seq_len)

if __name__ == '__main__':
    # When executed directly, run a tiny demo using the local train.csv if available
    print("Running tiny demo forward pass (no heavy training)...\\n")
    demo_small_forward_pass(train_csv_path='/mnt/data/emotion_tom_lembong/train.csv')
    print('\\nDemo finished.\\n')

import pandas as pd
import torch
from torch.utils.data import Dataset
from sklearn.metrics import f1_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments

# 1. Load dataset
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')
sample_sub = pd.read_csv('sample_submission.csv')

# 2. Label mapping (gunakan urutan dari sample submission agar konsisten)
label_order = sample_sub["label"].unique().tolist()
label2id = {label: i for i, label in enumerate(label_order)}
id2label = {i: label for label, i in label2id.items()}

train_texts = train_df["text"].tolist()
train_labels = train_df["label"].map(label2id).tolist()

# 3. Dataset class
class EmotionDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        enc = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )
        item = {k: v.squeeze() for k, v in enc.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

# 4. Tokenizer & model
tokenizer = AutoTokenizer.from_pretrained("indobenchmark/indobert-base-p2")
model = AutoModelForSequenceClassification.from_pretrained(
    "indobenchmark/indobert-base-p2",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

train_dataset = EmotionDataset(train_texts, train_labels, tokenizer)

# 5. Compute metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    return {"macro_f1": f1_score(labels, preds, average="macro")}

# 6. Trainer setup
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=16,
    num_train_epochs=2,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_steps=50,
    evaluate_during_training=False,  # versi lama
    disable_tqdm=True                # opsional, biar lebih bersih
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    compute_metrics=compute_metrics,
)

# 7. Train model
trainer.train()

# 8. Predict on test
test_encodings = tokenizer(
    test_df["text"].tolist(),
    truncation=True,
    padding="max_length",
    max_length=128,
    return_tensors="pt"
)
with torch.no_grad():
    outputs = model(**test_encodings)
    preds = outputs.logits.argmax(dim=-1).cpu().numpy()

pred_labels = [id2label[p] for p in preds]

# 9. Save submission (format sesuai sample submission)
submission = pd.DataFrame({
    "id_comment": test_df["id_comment"],
    "label": pred_labels
})

submission.to_csv("submission.csv", index=False)
print("âœ… Submission saved as submission.csv")

"""## EKSPERIMEN 3"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

!pip install -q transformers

# check for the GPU
!nvidia-smi

train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# check the size of the data
print("Train data size \n", train_df.shape )
print()
print("Test data size \n", test_df.shape )

train_df.head()

train_df.duplicated().sum()

# checking the target distribution
train_df.label.value_counts().plot(kind = 'bar')

for i in range(5):
  print(train_df[train_df.label=='SADNESS']['text'].iloc[i])

textlen = train_df['text'].apply(lambda x: len(x.split()))

import seaborn as sns
plt.figure(figsize=(8,5))
plt.hist(textlen, edgecolor = "black");

SEQ_LEN = 105

from transformers import BertTokenizer
from transformers import AutoTokenizer
import tensorflow as tf

from transformers import BertTokenizer

# Ini akan mengunduh semua file ke cache lokal
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

!ls -R /root/.cache/huggingface/hub/

from transformers import BertTokenizer
import os

# Ganti 'path/ke/folder/model' dengan lokasi folder di komputer Anda
local_model_path = "bert-base-uncased"

if os.path.isdir(local_model_path):
    tokenizer = BertTokenizer.from_pretrained(local_model_path, do_lower_case=True)
else:
    print(f"Error: Folder '{local_model_path}' tidak ditemukan.")

# download the tokenizer
task = "sentiment"
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Preprocess text (username and link placeholders)
def preprocess(text):
    new_text = []

    for t in text.split(" "):
        t = '' if t.startswitah('@') and len(t) > 1 else t
        t = '' if t.startswith('http') else t
        t = t.replace("#","")
        new_text.append(t.lower())
    return " ".join(new_text).strip().replace("  ", " ")

train_df['text'] = train_df['text'].apply(preprocess)
train_df.head()

"""TOKENIZER"""

# example of tokenization
tokenized_text = train_df['text'].apply(tokenizer.tokenize)
display(tokenized_text.head())

"""Tanda ## (hash-hash) adalah penanda bahwa unit tersebut adalah lanjutan dari kata sebelumnya.

Jadi, tanda ## bukan untuk mengecilkan huruf, tetapi untuk menunjukkan struktur sub-kata.
"""

# example of an output from encoded text by tokeneizer
encoded_input = tokenizer(train_df['text'].iloc[0], return_tensors='pt')
print(encoded_input)

"""- mengubah satu baris teks dari DataFrame menjadi format numerik yang bisa diproses oleh model PyTorch.
- return_tensors='pt': Argumen ini memberi tahu tokenizer untuk mengembalikan hasil dalam format tensor PyTorch ('pt'). Ini sangat penting karena model yang akan memproses input ini (seperti model BERT) menerima input dalam bentuk tensor.

Ubah kembali menjadi string
"""

tokenizer.decode(encoded_input['input_ids'][0])

""" FINALISASI TOKENIZER

 Mengubah seluruh kolom teks dalam DataFrame menjadi format numerik yang siap diproses oleh model TensorFlow
"""

# encoding the training data
train_encoded_inputs = tokenizer(train_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

train_encoded_inputs

"""Membuat Tensorflow Data"""

# Map string labels to integers
label_map = {label: i for i, label in enumerate(train_df['label'].unique())}
train_df['label_encoded'] = train_df['label'].map(label_map)

train_dataset = tf.data.Dataset.from_tensor_slices((train_encoded_inputs, train_df['label_encoded'].values))

# formatting the data as required by bert model
def map_bert(inputs, labels):
  inputs = {'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask']}

  return inputs, labels

train_dataset = train_dataset.map(map_bert)

for t in train_dataset.take(2):
  print (t)

"""Train Test Split"""

dataset = train_dataset.shuffle(100000).batch(64)

# length of the dataset(total batches)
DS_LEN = len(dataset)
DS_LEN

# take 80% for train and 20% for validation
SPLIT = 0.8
train_ds = dataset.take(round(DS_LEN*SPLIT))
val_ds = dataset.skip(round(DS_LEN*SPLIT))

"""Modelling"""

from transformers import TFAutoModel

bert = TFAutoModel.from_pretrained('bert-base-uncased')

# create model architecture
n_classes = len(train_df['label'].unique()) # Get the number of unique labels

# Input layers
input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), dtype=np.int32, name='input_ids' )
mask = tf.keras.layers.Input(shape=(SEQ_LEN,), dtype=np.int32, name = 'attention_mask')

# bert embeddings
embeddings = bert([input_ids, mask])[0]
cls_token = embeddings[:,0,:]

# keras layers
#x = tf.keras.layers.GlobalMaxPool1D()(embeddings)
x = tf.keras.layers.BatchNormalization()(cls_token)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(32, activation='relu')(x)

# output layer - changed to n_classes units and softmax activation
y = tf.keras.layers.Dense(n_classes, activation='softmax')(x)

# create the model
model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)

model.summary()

# freezing the pretrained bert layer
model.layers[2].trainable = False
model.summary()

"""COMPILE MODEL"""

import transformers

train_df['label'].unique()

#learning_rate = 1e-3

#optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)
optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-4)
#optimizer = transformers.AdamWeightDecay(learning_rate=5e-4)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

"""- min_delta pada ReduceLROnPlateau berfungsi sebagai ambang batas untuk menentukan kapan harus mengurangi learning rate.
- min_delta pada EarlyStopping berfungsi sebagai ambang batas untuk menghentikan pelatihan sepenuhnya ketika tidak ada perbaikan yang signifikan.
"""

# train the model
callbacks = [tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.1,min_delta=0.001,monitor='val_loss'),
             tf.keras.callbacks.EarlyStopping(patience=5, min_delta=0.01, monitor='val_loss')]

# Changed loss to SparseCategoricalCrossentropy for multi-class classification
loss = tf.keras.losses.SparseCategoricalCrossentropy()
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy') # Changed metric

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])


history = model.fit(
    train_ds,
    validation_data = val_ds,
    epochs = 50,
    callbacks = callbacks
)

# plot the performance curve
epochs = history.epoch
plt.figure(figsize=(15, 6))

# Accuracy
plt.subplot(1,2, 1)
plt.plot(epochs, history.history['accuracy'], label="Train")
plt.plot(epochs, history.history['val_accuracy'], label = "Val")
plt.legend()
plt.title("Accuracy")

# loss
plt.subplot(1,2, 2)
plt.plot(epochs, history.history['loss'], label="Train")
plt.plot(epochs, history.history['val_loss'], label = "Val")
plt.legend()
plt.title("Loss")

plt.show()

# evaluate on val data
model.evaluate(val_ds)

from sklearn.metrics import f1_score
import numpy as np

# Get true labels and predictions for the validation dataset
y_true = []
y_pred = []

for inputs, labels in val_ds:
    # Get predictions from the model
    logits = model.predict(inputs)
    predictions = np.argmax(logits, axis=1)

    # Append true labels and predictions
    y_true.extend(labels.numpy())
    y_pred.extend(predictions)

# Calculate macro F1 score
f1 = f1_score(y_true, y_pred, average='macro')
print("Macro F1 Score:", f1)

# Process the test data
test_df['text'] = test_df['text'].apply(preprocess) # Assuming preprocess function is defined

test_encoded_inputs = tokenizer(test_df['text'].tolist(),
                                add_special_tokens = True,
                                padding='max_length',
                                truncation=True,
                                max_length=SEQ_LEN,
                                return_token_type_ids=False,
                                return_tensors = 'tf')

# Format the test dataset for prediction
test_dataset = tf.data.Dataset.from_tensor_slices(test_encoded_inputs)

# Map the dataset to the expected format for model.predict
def map_test_bert(inputs):
    return {'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask']}

test_dataset = test_dataset.map(map_test_bert).batch(64)

# Make predictions on the test data
predictions = model.predict(test_dataset)

# Convert predictions to label names
# Assuming label_map is available from the training data processing
id_to_label = {i: label for label, i in label_map.items()}
predicted_labels = [id_to_label[np.argmax(p)] for p in predictions]

# Create submission DataFrame
submission_df = pd.DataFrame({'id_comment': test_df['id_comment'], 'label': predicted_labels})

# Save submission file
submission_df.to_csv('kumpu1.csv', index=False)

print("Submission file 'kumpu1.csv' created successfully!")

# Process the test data
test_df['text'] = test_df['text'].apply(preprocess) # Assuming preprocess function is defined

test_encoded_inputs = tokenizer(test_df['text'].tolist(),
                                add_special_tokens = True,
                                padding='max_length',
                                truncation=True,
                                max_length=SEQ_LEN,
                                return_token_type_ids=False,
                                return_tensors = 'tf')

# Format the test dataset for prediction
test_dataset = tf.data.Dataset.from_tensor_slices(test_encoded_inputs)

# Map the dataset to the expected format for model.predict
def map_test_bert(inputs):
    return {'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask']}

test_dataset = test_dataset.map(map_test_bert).batch(64)

# Process the test data
# ... (kode di atas tetap sama) ...

# Make predictions on the test data
predictions = model.predict(test_dataset)

# Ambil indeks kelas dengan probabilitas tertinggi dari hasil prediksi
predicted_indices = np.argmax(predictions, axis=1)

# Ubah indeks ke dalam format list Python
predicted_indices_list = predicted_indices.tolist()

# Create submission DataFrame with numerical labels
submission_df = pd.DataFrame({'id_comment': test_df['id_comment'], 'label': predicted_indices_list})

# Save submission file
submission_df.to_csv('kumpul2.csv', index=False)

print("Submission file 'kumpul2.csv' created successfully with numerical labels!")

"""### MODIFIKASI KENAIKAN F1 SCORE"""



"""### MODIFIKASI KENAIKAN F1 SCORE (1) - AdamW"""

#learning_rate = 1e-3

#optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)
optimizer = tf.keras.optimizers.AdamW(learning_rate=3e-5)
#optimizer = transformers.AdamWeightDecay(learning_rate=5e-4)
loss = tf.keras.losses.BinaryCrossentropy()
metric = tf.keras.metrics.BinaryAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# train the model
callbacks = [tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.1,min_delta=0.001,monitor='val_loss'),
             tf.keras.callbacks.EarlyStopping(patience=5, min_delta=0.01, monitor='val_loss')]

# Changed loss to SparseCategoricalCrossentropy for multi-class classification
loss = tf.keras.losses.SparseCategoricalCrossentropy()
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy') # Changed metric

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])


history = model.fit(
    train_ds,
    validation_data = val_ds,
    epochs = 50,
    callbacks = callbacks
)

"""### MODIFIKASI KENAIKAN F1 SCORE (1) - AdamW"""

# create model architecture
n_classes = len(train_df['label'].unique()) # Get the number of unique labels

# Input layers
input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), dtype=np.int32, name='input_ids' )
mask = tf.keras.layers.Input(shape=(SEQ_LEN,), dtype=np.int32, name = 'attention_mask')

# bert embeddings
embeddings = bert([input_ids, mask])[0]
cls_token = embeddings[:,0,:]

# keras layers
x = tf.keras.layers.Dropout(0.2)(cls_token)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(32, activation='relu')(x)

# output layer - changed to n_classes units and softmax activation
y = tf.keras.layers.Dense(n_classes)(x)

# create the model
model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)

# freezing the pretrained bert layer
model.layers[2].trainable = False
model.summary()

# Hapus optimizer lama, gunakan AdamW dari `transformers`
optimizer = transformers.AdamWeightDecay(learning_rate=3e-5)

# Gunakan loss dan metrik yang sesuai
# Jika masalahnya multi-class classification
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# train the model
callbacks = [tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.1,min_delta=0.001,monitor='val_loss'),
             tf.keras.callbacks.EarlyStopping(patience=5, min_delta=0.01, monitor='val_loss')]

# Ubah loss function dan metrik agar sesuai dengan arsitektur model
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Lanjutkan dengan pelatihan
history = model.fit(
    train_ds,
    validation_data = val_ds,
    epochs = 50,
    callbacks = callbacks
)

from sklearn.metrics import f1_score
import numpy as np

# Get true labels and predictions for the validation dataset
y_true = []
y_pred = []

for inputs, labels in val_ds:
    # Get predictions from the model
    logits = model.predict(inputs)
    predictions = np.argmax(logits, axis=1)

    # Append true labels and predictions
    y_true.extend(labels.numpy())
    y_pred.extend(predictions)

# Calculate macro F1 score
f1 = f1_score(y_true, y_pred, average='macro')
print("Macro F1 Score:", f1)



"""# EKSPERIMEN 4"""

import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split

# Load data
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Define the preprocess function (assuming it's the same as in Experiment 3)
def preprocess(text):
    new_text = []
    for t in str(text).split(" "): # Ensure text is string
        t = '' if t.startswith('@') and len(t) > 1 else t
        t = '' if t.startswith('http') else t
        t = t.replace("#","")
        new_text.append(t.lower())
    return " ".join(new_text).strip().replace("  ", " ")

# Apply preprocessing
train_df['text'] = train_df['text'].apply(preprocess)
test_df['text'] = test_df['text'].apply(preprocess)


# Map string labels to integers (0, 1, 2, 3, 4)
# Using the order from the most frequent labels to less frequent from Experiment 1
# SADNESS, ANGER, SUPPORT, HOPE, DISAPPOINTMENT
label_order = ['SADNESS', 'ANGER', 'SUPPORT', 'HOPE', 'DISAPPOINTMENT']
label_map = {label: i for i, label in enumerate(label_order)}
id_to_label = {i: label for label, i in label_map.items()} # For converting back to string labels later


train_df['label_encoded'] = train_df['label'].map(label_map)

# Define sequence length
SEQ_LEN = 105 # Based on analysis in Experiment 3

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Encode training data
train_encoded_inputs = tokenizer(train_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

# Create TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((train_encoded_inputs, train_df['label_encoded'].values))

# Format data for BERT model
def map_bert(inputs, labels):
  inputs = {'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask']}
  return inputs, labels

train_dataset = train_dataset.map(map_bert)

# Split dataset for training and validation
DS_LEN = len(train_dataset)
SPLIT = 0.8
train_ds = train_dataset.take(round(DS_LEN * SPLIT)).batch(64)
val_ds = train_dataset.skip(round(DS_LEN * SPLIT)).batch(64)


# Load BERT model for sequence classification
n_classes = len(label_order) # Use the defined order for number of classes
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_classes)

# Compile model
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) # Using Adam optimizer
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Train model (using fewer epochs for demonstration)
print("Starting model training...")
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=5, # Reduced number of epochs
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')] # Added early stopping
)
print("Training finished.")

# Process test data
test_encoded_inputs = tokenizer(test_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

test_dataset = tf.data.Dataset.from_tensor_slices(test_encoded_inputs)
test_dataset = test_dataset.map(lambda x: {'input_ids': x['input_ids'], 'attention_mask': x['attention_mask']}).batch(64)


# Make predictions on the test data
print("Making predictions on test data...")
predictions = model.predict(test_dataset)

# Get predicted labels (indices)
predicted_indices = np.argmax(predictions.logits, axis=1)

# Create submission DataFrame with numerical labels (0, 1, 2, 3, 4)
submission_df = pd.DataFrame({'id_comment': test_df['id_comment'], 'label': predicted_indices})

# Save submission file
submission_df.to_csv('submission1.csv', index=False)

print("\nSubmission file 'submission1.csv' created successfully with numerical labels!")

import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split

# Load data
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Define the preprocess function (assuming it's the same as in Experiment 3)
def preprocess(text):
    new_text = []
    for t in str(text).split(" "): # Ensure text is string
        t = '' if t.startswith('@') and len(t) > 1 else t
        t = '' if t.startswith('http') else t
        t = t.replace("#","")
        new_text.append(t.lower())
    return " ".join(new_text).strip().replace("  ", " ")

# Apply preprocessing
train_df['text'] = train_df['text'].apply(preprocess)
test_df['text'] = test_df['text'].apply(preprocess)


# Map string labels to integers (0, 1, 2, 3, 4)
# Using the order from the most frequent labels to less frequent from Experiment 1
# SADNESS, ANGER, SUPPORT, HOPE, DISAPPOINTMENT
label_order = ['SADNESS', 'ANGER', 'SUPPORT', 'HOPE', 'DISAPPOINTMENT']
label_map = {label: i for i, label in enumerate(label_order)}
id_to_label = {i: label for label, i in label_map.items()} # For converting back to string labels later


train_df['label_encoded'] = train_df['label'].map(label_map)

# Define sequence length
SEQ_LEN = 105 # Based on analysis in Experiment 3

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Encode training data
train_encoded_inputs = tokenizer(train_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

# Create TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((train_encoded_inputs, train_df['label_encoded'].values))

# Format data for BERT model
def map_bert(inputs, labels):
  inputs = {'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask']}
  return inputs, labels

train_dataset = train_dataset.map(map_bert)

# Split dataset for training and validation
DS_LEN = len(train_dataset)
SPLIT = 0.8
train_ds = train_dataset.take(round(DS_LEN * SPLIT)).batch(64)
val_ds = train_dataset.skip(round(DS_LEN * SPLIT)).batch(64)


# Load BERT model for sequence classification
n_classes = len(label_order) # Use the defined order for number of classes
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_classes)

# Compile model
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) # Using Adam optimizer
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Train model (using fewer epochs for demonstration)
print("Starting model training...")
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=5, # Reduced number of epochs
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')] # Added early stopping
)
print("Training finished.")

# Process test data
test_encoded_inputs = tokenizer(test_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

test_dataset = tf.data.Dataset.from_tensor_slices(test_encoded_inputs)
test_dataset = test_dataset.map(lambda x: {'input_ids': x['input_ids'], 'attention_mask': x['attention_mask']}).batch(64)


# Make predictions on the test data
print("Making predictions on test data...")
predictions = model.predict(test_dataset)

# Get predicted labels (indices)
predicted_indices = np.argmax(predictions.logits, axis=1)

try:
    test_labels_df = pd.read_csv('test_labels.csv')
    test_labels_df['label_encoded'] = test_labels_df['label'].map(label_map)
    y_true = test_labels_df['label_encoded'].values

    # Calculate F1 score
    f1 = f1_score(y_true, predicted_indices, average='weighted')
    print(f"\nFinal Weighted F1 Score: {f1:.4f}")

except FileNotFoundError:
    print("\nWarning: Could not find 'test_labels.csv'. F1 score cannot be calculated.")

# Create submission DataFrame with numerical labels (0, 1, 2, 3, 4)
submission_df = pd.DataFrame({'id_comment': test_df['id_comment'], 'label': predicted_indices})

# Save submission file
submission_df.to_csv('submission2.csv', index=False)

print("\nSubmission file 'submission2.csv' created successfully with numerical labels!")

import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split

# Load data
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Define the preprocess function (assuming it's the same as in Experiment 3)
def preprocess(text):
    new_text = []
    for t in str(text).split(" "): # Ensure text is string
        t = '' if t.startswith('@') and len(t) > 1 else t
        t = '' if t.startswith('http') else t
        t = t.replace("#","")
        new_text.append(t.lower())
    return " ".join(new_text).strip().replace("  ", " ")

# Apply preprocessing
train_df['text'] = train_df['text'].apply(preprocess)
test_df['text'] = test_df['text'].apply(preprocess)


# Map string labels to integers (0, 1, 2, 3, 4)
# Using the order from the most frequent labels to less frequent from Experiment 1
# SADNESS, ANGER, SUPPORT, HOPE, DISAPPOINTMENT
label_order = ['SADNESS', 'ANGER', 'SUPPORT', 'HOPE', 'DISAPPOINTMENT']
label_map = {label: i for i, label in enumerate(label_order)}
id_to_label = {i: label for label, i in label_map.items()} # For converting back to string labels later


train_df['label_encoded'] = train_df['label'].map(label_map)

# Define sequence length
SEQ_LEN = 105 # Based on analysis in Experiment 3

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Encode training data
train_encoded_inputs = tokenizer(train_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

# Create TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((train_encoded_inputs, train_df['label_encoded'].values))

# Format data for BERT model
def map_bert(inputs, labels):
  inputs = {'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask']}
  return inputs, labels

train_dataset = train_dataset.map(map_bert)

# Split dataset for training and validation
DS_LEN = len(train_dataset)
SPLIT = 0.8
train_ds = train_dataset.take(round(DS_LEN * SPLIT)).batch(64)
val_ds = train_dataset.skip(round(DS_LEN * SPLIT)).batch(64)


# Load BERT model for sequence classification
n_classes = len(label_order) # Use the defined order for number of classes
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_classes)

# Compile model
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) # Using Adam optimizer
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Train model (using fewer epochs for demonstration)
print("Starting model training...")
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=5, # Reduced number of epochs
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')] # Added early stopping
)
print("Training finished.")

# Process test data
test_encoded_inputs = tokenizer(test_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

test_dataset = tf.data.Dataset.from_tensor_slices(test_encoded_inputs)
test_dataset = test_dataset.map(lambda x: {'input_ids': x['input_ids'], 'attention_mask': x['attention_mask']}).batch(64)


# Make predictions on the test data
print("Making predictions on test data...")
predictions = model.predict(test_dataset)

# Get predicted labels (indices)
predicted_indices = np.argmax(predictions.logits, axis=1)

try:
    test_labels_df = pd.read_csv('test_labels.csv')
    test_labels_df['label_encoded'] = test_labels_df['label'].map(label_map)
    y_true = test_labels_df['label_encoded'].values

    # Calculate F1 score
    f1 = f1_score(y_true, predicted_indices, average='weighted')
    print(f"\nFinal Weighted F1 Score: {f1:.4f}")

except FileNotFoundError:
    print("\nWarning: Could not find 'test_labels.csv'. F1 score cannot be calculated.")

# Create submission DataFrame with numerical labels (0, 1, 2, 3, 4)
submission_df = pd.DataFrame({'id_comment': test_df['id_comment'], 'label': predicted_indices})

# Save submission file
submission_df.to_csv('submission2.csv', index=False)

print("\nSubmission file 'submission2.csv' created successfully with numerical labels!")

# Save the model in H5 format
try:
    # Use the save() method to save the entire model
    # (architecture, weights, and optimizer state)
    model.save('bert_finetuned_model.h5')
    print("\nModel saved successfully as 'bert_finetuned_model.h5'!")
except Exception as e:
    print(f"\nAn error occurred while saving the model: {e}")

"""### COBA 2"""

import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split

# Load data
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Define the preprocess function (assuming it's the same as in Experiment 3)
def preprocess(text):
    new_text = []
    for t in str(text).split(" "): # Ensure text is string
        t = '' if t.startswith('@') and len(t) > 1 else t
        t = '' if t.startswith('http') else t
        t = t.replace("#","")
        new_text.append(t.lower())
    return " ".join(new_text).strip().replace("  ", " ")

# Apply preprocessing
train_df['text'] = train_df['text'].apply(preprocess)
test_df['text'] = test_df['text'].apply(preprocess)


# Map string labels to integers (0, 1, 2, 3, 4)
# Using the order from the most frequent labels to less frequent from Experiment 1
# SADNESS, ANGER, SUPPORT, HOPE, DISAPPOINTMENT
label_order = ['SADNESS', 'ANGER', 'SUPPORT', 'HOPE', 'DISAPPOINTMENT']
label_map = {label: i for i, label in enumerate(label_order)}
id_to_label = {i: label for label, i in label_map.items()} # For converting back to string labels later


train_df['label_encoded'] = train_df['label'].map(label_map)

# Define sequence length
SEQ_LEN = 105 # Based on analysis in Experiment 3

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Encode training data
train_encoded_inputs = tokenizer(train_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

# Create TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((train_encoded_inputs, train_df['label_encoded'].values))

# Format data for BERT model
def map_bert(inputs, labels):
  inputs = {'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask']}
  return inputs, labels

train_dataset = train_dataset.map(map_bert)

# Split dataset for training and validation
DS_LEN = len(train_dataset)
SPLIT = 0.8
# Reduced batch size to prevent OOM
BATCH_SIZE = 32
train_ds = train_dataset.take(round(DS_LEN * SPLIT)).batch(BATCH_SIZE)
val_ds = train_dataset.skip(round(DS_LEN * SPLIT)).batch(BATCH_SIZE)


# Load BERT model for sequence classification
n_classes = len(label_order) # Use the defined order for number of classes
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_classes)

# Compile model
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) # Using Adam optimizer
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Train model (using fewer epochs for demonstration)
print("Starting model training...")
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=5, # Reduced number of epochs
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')] # Added early stopping
)
print("Training finished.")

# --- Plot the performance curve ---
epochs = history.epoch
plt.figure(figsize=(15, 6))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs, history.history['accuracy'], label="Train")
plt.plot(epochs, history.history['val_accuracy'], label="Val")
plt.legend()
plt.title("Accuracy")

# Loss
plt.subplot(1, 2, 2)
plt.plot(epochs, history.history['loss'], label="Train")
plt.plot(epochs, history.history['val_loss'], label="Val")
plt.legend()
plt.title("Loss")

plt.show()

# Process test data
test_encoded_inputs = tokenizer(test_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

test_dataset = tf.data.Dataset.from_tensor_slices(test_encoded_inputs)
# Reduced batch size for test dataset as well
test_dataset = test_dataset.map(lambda x: {'input_ids': x['input_ids'], 'attention_mask': x['attention_mask']}).batch(BATCH_SIZE)


# Make predictions on the test data
print("Making predictions on test data...")
predictions = model.predict(test_dataset)

# Get predicted labels (indices)
predicted_indices = np.argmax(predictions.logits, axis=1)

try:
    test_labels_df = pd.read_csv('test.csv')
    test_labels_df['label_encoded'] = test_labels_df['label'].map(label_map)
    y_true = test_labels_df['label_encoded'].values

    # Calculate F1 score
    f1 = f1_score(y_true, predicted_indices, average='weighted')
    print(f"\nFinal Weighted F1 Score: {f1:.4f}")

except FileNotFoundError:
    print("\nWarning: Could not find 'test_labels.csv'. F1 score cannot be calculated.")
except KeyError:
    print("\nWarning: 'label' column not found in 'test.csv'. F1 score cannot be calculated.")


# Create submission DataFrame with numerical labels (0, 1, 2, 3, 4)
submission_df = pd.DataFrame({'id_comment': test_df['id_comment'], 'label': predicted_indices})

# Save submission file
submission_df.to_csv('submission3.csv', index=False)

print("\nSubmission file 'submission3.csv' created successfully with numerical labels!")

"""### COBA 3"""

import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report
import matplotlib.pyplot as plt

# Load data
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Define the preprocess function
def preprocess(text):
    new_text = []
    for t in str(text).split(" "): # Ensure text is string
        t = '' if t.startswith('@') and len(t) > 1 else t
        t = '' if t.startswith('http') else t
        t = t.replace("#","")
        new_text.append(t.lower())
    return " ".join(new_text).strip().replace("  ", " ")

# Apply preprocessing
train_df['text'] = train_df['text'].apply(preprocess)
test_df['text'] = test_df['text'].apply(preprocess)

# Map string labels to integers
label_order = ['SADNESS', 'ANGER', 'SUPPORT', 'HOPE', 'DISAPPOINTMENT']
label_map = {label: i for i, label in enumerate(label_order)}
id_to_label = {i: label for label, i in label_map.items()}

train_df['label_encoded'] = train_df['label'].map(label_map)

# Define sequence length
SEQ_LEN = 105

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Encode training data
train_encoded_inputs = tokenizer(train_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

# Create TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((train_encoded_inputs, train_df['label_encoded'].values))

# Format data for BERT model
def map_bert(inputs, labels):
  inputs = {'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask']}
  return inputs, labels

train_dataset = train_dataset.map(map_bert)

# Split dataset
DS_LEN = len(train_dataset)
SPLIT = 0.8
BATCH_SIZE = 32
train_ds = train_dataset.take(round(DS_LEN * SPLIT)).batch(BATCH_SIZE)
val_ds = train_dataset.skip(round(DS_LEN * SPLIT)).batch(BATCH_SIZE)

# Load BERT model
n_classes = len(label_order)
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_classes)

# Compile
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Train
print("Starting model training...")
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=5,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)]
)
print("Training finished.")

# --- Evaluate on Validation Set ---
y_true = []
y_pred = []

for batch in val_ds:
    inputs, labels = batch
    preds = model.predict(inputs).logits
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(preds, axis=1))

print("\nClassification Report (Validation Set):")
print(classification_report(y_true, y_pred, target_names=label_order))

# --- Evaluate on Validation Set ---
y_true = []
y_pred = []

for batch in val_ds:
    inputs, labels = batch
    preds = model.predict(inputs).logits
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(preds, axis=1))

print("\nClassification Report (Validation Set):")
print(classification_report(y_true, y_pred, target_names=label_order))

# --- Tambahan: Macro F1 Score ---
f1 = f1_score(y_true, y_pred, average='macro')
print("Macro F1 Score:", f1)

# --- Plot curves ---
epochs = history.epoch
plt.figure(figsize=(15, 6))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs, history.history['accuracy'], label="Train")
plt.plot(epochs, history.history['val_accuracy'], label="Val")
plt.legend()
plt.title("Accuracy")

# Loss
plt.subplot(1, 2, 2)
plt.plot(epochs, history.history['loss'], label="Train")
plt.plot(epochs, history.history['val_loss'], label="Val")
plt.legend()
plt.title("Loss")

plt.show()

# --- Predict Test Data (no label, only for submission) ---
test_encoded_inputs = tokenizer(test_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

test_dataset = tf.data.Dataset.from_tensor_slices(test_encoded_inputs)
test_dataset = test_dataset.map(lambda x: {'input_ids': x['input_ids'], 'attention_mask': x['attention_mask']}).batch(BATCH_SIZE)

print("Making predictions on test data...")
predictions = model.predict(test_dataset)
predicted_indices = np.argmax(predictions.logits, axis=1)

# Create submission
submission_df = pd.DataFrame({'id_comment': test_df['id_comment'], 'label': predicted_indices})
submission_df.to_csv('submission4.csv', index=False)
print("\nSubmission file 'submission4.csv' created successfully with numerical labels!")

"""# Recode"""

import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split

train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

def preprocess(text):
    new_text = []
    for t in str(text).split(" "):
        t = '' if t.startswith('@') and len(t) > 1 else t
        t = '' if t.startswith('http') else t
        t = t.replace("#","")
        new_text.append(t.lower())
    return " ".join(new_text).strip().replace("  ", " ")

train_df['text'] = train_df['text'].apply(preprocess)
test_df['text'] = test_df['text'].apply(preprocess)

label_order = ['SADNESS', 'ANGER', 'SUPPORT', 'HOPE', 'DISAPPOINTMENT']
label_map = {label: i for i, label in enumerate(label_order)}
id_to_label = {i: label for label, i in label_map.items()}

train_df['label_encoded'] = train_df['label'].map(label_map)

SEQ_LEN = 105

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

train_encoded_inputs = tokenizer(train_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

train_dataset = tf.data.Dataset.from_tensor_slices((train_encoded_inputs, train_df['label_encoded'].values))

def map_bert(inputs, labels):
  inputs = {'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask']}
  return inputs, labels

train_dataset = train_dataset.map(map_bert)

DS_LEN = len(train_dataset)
SPLIT = 0.8
train_ds = train_dataset.take(round(DS_LEN * SPLIT)).batch(64)
val_ds = train_dataset.skip(round(DS_LEN * SPLIT)).batch(64)

n_classes = len(label_order) # Use the defined order for number of classes
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_classes)

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) # Using Adam optimizer
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

from sklearn.utils import class_weight
import numpy as np

# Calculate class weights
class_weights = class_weight.compute_class_weight(
    'balanced',
    classes=np.unique(train_df['label_encoded']),
    y=train_df['label_encoded']
)

# Convert class weights to a dictionary
class_weights_dict = dict(enumerate(class_weights))

print("Class weights:", class_weights_dict)

# Recompile the model with class weights
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) # Using Adam optimizer
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Train the model with class weights
print("Starting model training with class weights...")
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')],
    class_weight=class_weights_dict # Add class weights here
)
print("Training finished.")

