# -*- coding: utf-8 -*-
"""MBC Week 4 F

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ILeJ_m2sC5pHawVDe_JY5AyU80mQpIRJ
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score

# Load data
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Define the preprocess function (assuming it's the same as in Experiment 3)
def preprocess(text):
    new_text = []
    for t in str(text).split(" "): # Ensure text is string
        t = '' if t.startswith('@') and len(t) > 1 else t
        t = '' if t.startswith('http') else t
        t = t.replace("#","")
        new_text.append(t.lower())
    return " ".join(new_text).strip().replace("  ", " ")

# Apply preprocessing
train_df['text'] = train_df['text'].apply(preprocess)
test_df['text'] = test_df['text'].apply(preprocess)


# Map string labels to integers (0, 1, 2, 3, 4)
# Using the order from the most frequent labels to less frequent from Experiment 1
# SADNESS, ANGER, SUPPORT, HOPE, DISAPPOINTMENT
label_order = ['SADNESS', 'ANGER', 'SUPPORT', 'HOPE', 'DISAPPOINTMENT']
label_map = {label: i for i, label in enumerate(label_order)}
id_to_label = {i: label for label, i in label_map.items()} # For converting back to string labels later


train_df['label_encoded'] = train_df['label'].map(label_map)

# Define sequence length
SEQ_LEN = 105 # Based on analysis in Experiment 3

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, cache_dir=".cache")

# Encode training data
train_encoded_inputs = tokenizer(train_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

# Create TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((train_encoded_inputs, train_df['label_encoded'].values))

# Format data for BERT model
def map_bert(inputs, labels):
  inputs = {'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask']}
  return inputs, labels

train_dataset = train_dataset.map(map_bert)

# Split dataset for training and validation
DS_LEN = len(train_dataset)
SPLIT = 0.8
train_ds = train_dataset.take(round(DS_LEN * SPLIT)).batch(64)
val_ds = train_dataset.skip(round(DS_LEN * SPLIT)).batch(64)


# Load BERT model for sequence classification
n_classes = len(label_order) # Use the defined order for number of classes
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_classes)

# Compile model
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) # Using Adam optimizer
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Train model (using fewer epochs for demonstration)
print("Starting model training...")
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=5, # Reduced number of epochs
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')] # Added early stopping
)
print("Training finished.")

# Process test data
test_encoded_inputs = tokenizer(test_df['text'].tolist(),
                                 add_special_tokens = True,
                                 padding='max_length',
                                 truncation=True,
                                 max_length=SEQ_LEN,
                                 return_token_type_ids=False,
                                 return_tensors = 'tf')

test_dataset = tf.data.Dataset.from_tensor_slices(test_encoded_inputs)
test_dataset = test_dataset.map(lambda x: {'input_ids': x['input_ids'], 'attention_mask': x['attention_mask']}).batch(64)


# Make predictions on the test data
print("Making predictions on test data...")
predictions = model.predict(test_dataset)

# Get predicted labels (indices)
predicted_indices = np.argmax(predictions.logits, axis=1)

try:
    test_labels_df = pd.read_csv('test_labels.csv')
    test_labels_df['label_encoded'] = test_labels_df['label'].map(label_map)
    y_true = test_labels_df['label_encoded'].values

    # Calculate F1 score
    f1 = f1_score(y_true, predicted_indices, average='weighted')
    print(f"\nFinal Weighted F1 Score: {f1:.4f}")

except FileNotFoundError:
    print("\nWarning: Could not find 'test_labels.csv'. F1 score cannot be calculated.")

# Create submission DataFrame with numerical labels (0, 1, 2, 3, 4)
submission_df = pd.DataFrame({'id_comment': test_df['id_comment'], 'label': predicted_indices})

# Save submission file
submission_df.to_csv('submission2.csv', index=False)

print("\nSubmission file 'submission2.csv' created successfully with numerical labels!")
